"use strict";(self.webpackChunkblog_sample=self.webpackChunkblog_sample||[]).push([[9058],{5629:e=>{e.exports=JSON.parse('{"label":"paper","permalink":"/docs/tags/paper","allTagsPath":"/docs/tags","count":2,"items":[{"id":"LLM/paper/Transformer-MM-Explainability","title":"Generic Attention-model Explainability for Interpreting Bi-Modal and Encoder-Decoder Transformers","description":"\u8fd9\u662f\u4e00\u7bc7\u5173\u4e8e\u89e3\u91catransformer\u6ce8\u610f\u529b\u7684\u6587\u7ae0\uff0c\u601d\u8def\u8fd1\u4f3cGradCAM\uff0c\u4e3b\u8981\u662f\u53ef\u4ee5\u53d1\u73b0transformer\u7c7b\u7684\u6a21\u578b\u5728\u54ea\u4e00\u5757\u7f6e\u4fe1\u5ea6\u6bd4\u8f83\u9ad8\u3002","permalink":"/docs/LLM/paper/Transformer-MM-Explainability"},{"id":"LLM/paper/codeVQA","title":"Modular visual question answering via code generation","description":"Modular visual question answering via code generation","permalink":"/docs/LLM/paper/codeVQA"}],"unlisted":false}')}}]);