"use strict";(self.webpackChunkblog_sample=self.webpackChunkblog_sample||[]).push([[2309],{3905:(n,e,t)=>{t.d(e,{Zo:()=>_,kt:()=>p});var a=t(7294);function r(n,e,t){return e in n?Object.defineProperty(n,e,{value:t,enumerable:!0,configurable:!0,writable:!0}):n[e]=t,n}function c(n,e){var t=Object.keys(n);if(Object.getOwnPropertySymbols){var a=Object.getOwnPropertySymbols(n);e&&(a=a.filter((function(e){return Object.getOwnPropertyDescriptor(n,e).enumerable}))),t.push.apply(t,a)}return t}function i(n){for(var e=1;e<arguments.length;e++){var t=null!=arguments[e]?arguments[e]:{};e%2?c(Object(t),!0).forEach((function(e){r(n,e,t[e])})):Object.getOwnPropertyDescriptors?Object.defineProperties(n,Object.getOwnPropertyDescriptors(t)):c(Object(t)).forEach((function(e){Object.defineProperty(n,e,Object.getOwnPropertyDescriptor(t,e))}))}return n}function l(n,e){if(null==n)return{};var t,a,r=function(n,e){if(null==n)return{};var t,a,r={},c=Object.keys(n);for(a=0;a<c.length;a++)t=c[a],e.indexOf(t)>=0||(r[t]=n[t]);return r}(n,e);if(Object.getOwnPropertySymbols){var c=Object.getOwnPropertySymbols(n);for(a=0;a<c.length;a++)t=c[a],e.indexOf(t)>=0||Object.prototype.propertyIsEnumerable.call(n,t)&&(r[t]=n[t])}return r}var d=a.createContext({}),o=function(n){var e=a.useContext(d),t=e;return n&&(t="function"==typeof n?n(e):i(i({},e),n)),t},_=function(n){var e=o(n.components);return a.createElement(d.Provider,{value:e},n.children)},s="mdxType",u={inlineCode:"code",wrapper:function(n){var e=n.children;return a.createElement(a.Fragment,{},e)}},m=a.forwardRef((function(n,e){var t=n.components,r=n.mdxType,c=n.originalType,d=n.parentName,_=l(n,["components","mdxType","originalType","parentName"]),s=o(t),m=r,p=s["".concat(d,".").concat(m)]||s[m]||u[m]||c;return t?a.createElement(p,i(i({ref:e},_),{},{components:t})):a.createElement(p,i({ref:e},_))}));function p(n,e){var t=arguments,r=e&&e.mdxType;if("string"==typeof n||r){var c=t.length,i=new Array(c);i[0]=m;var l={};for(var d in e)hasOwnProperty.call(e,d)&&(l[d]=e[d]);l.originalType=n,l[s]="string"==typeof n?n:r,i[1]=l;for(var o=2;o<c;o++)i[o]=t[o];return a.createElement.apply(null,i)}return a.createElement.apply(null,t)}m.displayName="MDXCreateElement"},7818:(n,e,t)=>{t.r(e),t.d(e,{assets:()=>d,contentTitle:()=>i,default:()=>u,frontMatter:()=>c,metadata:()=>l,toc:()=>o});var a=t(7462),r=(t(7294),t(3905));const c={title:"cuda stream",tags:["work"]},i=void 0,l={unversionedId:"cuda/cuda_stream",id:"cuda/cuda_stream",title:"cuda stream",description:"Default Stream and Non-Default Blocking Stream",source:"@site/docs/cuda/cuda_stream.md",sourceDirName:"cuda",slug:"/cuda/cuda_stream",permalink:"/docs/cuda/cuda_stream",draft:!1,editUrl:"https://github.com/facebook/docusaurus/edit/main/website/docs/cuda/cuda_stream.md",tags:[{label:"work",permalink:"/docs/tags/work"}],version:"current",lastUpdatedAt:1699366301,formattedLastUpdatedAt:"Nov 7, 2023",frontMatter:{title:"cuda stream",tags:["work"]},sidebar:"tutorialSidebar",previous:{title:"GPU",permalink:"/docs/cuda/other"},next:{title:"element \u64cd\u4f5c",permalink:"/docs/cuda/element"}},d={},o=[{value:"Default Stream and Non-Default Blocking Stream",id:"default-stream-and-non-default-blocking-stream",level:2},{value:"Default Stream and Non-Default Non-Blocking Stream",id:"default-stream-and-non-default-non-blocking-stream",level:2}],_={toc:o},s="wrapper";function u(n){let{components:e,...t}=n;return(0,r.kt)(s,(0,a.Z)({},_,t,{components:e,mdxType:"MDXLayout"}),(0,r.kt)("h2",{id:"default-stream-and-non-default-blocking-stream"},"Default Stream and Non-Default Blocking Stream"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-cpp"},'#include <cassert>\n#include <iostream>\n#include <vector>\n\n#include <cuda_runtime.h>\n\n#define CHECK_CUDA_ERROR(val) check((val), #val, __FILE__, __LINE__)\ntemplate <typename T>\nvoid check(T err, const char* const func, const char* const file,\n           const int line)\n{\n    if (err != cudaSuccess)\n    {\n        std::cerr << "CUDA Runtime Error at: " << file << ":" << line\n                  << std::endl;\n        std::cerr << cudaGetErrorString(err) << " " << func << std::endl;\n        std::exit(EXIT_FAILURE);\n    }\n}\n\n#define CHECK_LAST_CUDA_ERROR() checkLast(__FILE__, __LINE__)\nvoid checkLast(const char* const file, const int line)\n{\n    cudaError_t err{cudaGetLastError()};\n    if (err != cudaSuccess)\n    {\n        std::cerr << "CUDA Runtime Error at: " << file << ":" << line\n                  << std::endl;\n        std::cerr << cudaGetErrorString(err) << std::endl;\n        std::exit(EXIT_FAILURE);\n    }\n}\n\n__global__ void add_val_in_place(int32_t* data, int32_t val, uint32_t n)\n{\n    uint32_t const idx{blockDim.x * blockIdx.x + threadIdx.x};\n    uint32_t const stride{blockDim.x * gridDim.x};\n    for (uint32_t i{idx}; i < n; i += stride)\n    {\n        data[i] += val;\n    }\n}\n\nvoid launch_add_val_in_place(int32_t* data, int32_t val, uint32_t n,\n                             cudaStream_t stream)\n{\n    dim3 const threads_per_block{1024};\n    dim3 const blocks_per_grid{32};\n    add_val_in_place<<<blocks_per_grid, threads_per_block, 0, stream>>>(data,\n                                                                        val, n);\n    CHECK_LAST_CUDA_ERROR();\n}\n\nbool check_array_value(int32_t const* data, uint32_t n, int32_t val)\n{\n    for (uint32_t i{0}; i < n; ++i)\n    {\n        if (data[i] != val)\n        {\n            return false;\n        }\n    }\n    return true;\n}\n\nint main()\n{\n    constexpr uint32_t const n{1000000};\n    constexpr int32_t const val_1{1};\n    constexpr int32_t const val_2{2};\n    constexpr int32_t const val_3{3};\n    // Create an multi-stream application.\n    cudaStream_t stream_1{0};\n    cudaStream_t stream_2{0};\n    // stream_1 is a non-default blocking stream.\n    CHECK_CUDA_ERROR(cudaStreamCreate(&stream_1));\n\n    std::vector<int32_t> vec(n, 0);\n    int32_t* d_data{nullptr};\n    CHECK_CUDA_ERROR(cudaMalloc(&d_data, n * sizeof(int32_t)));\n    CHECK_CUDA_ERROR(cudaMemcpy(d_data, vec.data(), n * sizeof(int32_t),\n                                cudaMemcpyHostToDevice));\n    // Run a sequence of CUDA kernels in order on the same CUDA stream.\n    launch_add_val_in_place(d_data, val_1, n, stream_1);\n    // The second kernel launch is supposed to be run on stream_1.\n    // However, the implementation has a typo such that the kernel launch\n    // is run on the default stream_2.\n    launch_add_val_in_place(d_data, val_2, n, stream_2);\n    launch_add_val_in_place(d_data, val_3, n, stream_1);\n\n    CHECK_CUDA_ERROR(cudaStreamSynchronize(stream_1));\n    CHECK_CUDA_ERROR(cudaMemcpy(vec.data(), d_data, n * sizeof(int32_t),\n                                cudaMemcpyDeviceToHost));\n\n    // Check the correctness of the application.\n    // Yet the result will still be correct if the default stream_2\n    // is a legacy default stream.\n    assert(check_array_value(vec.data(), n, val_1 + val_2 + val_3));\n\n    CHECK_CUDA_ERROR(cudaFree(d_data));\n    CHECK_CUDA_ERROR(cudaStreamDestroy(stream_1));\n}\n')),(0,r.kt)("h2",{id:"default-stream-and-non-default-non-blocking-stream"},"Default Stream and Non-Default Non-Blocking Stream"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-cpp"},'#include <cassert>\n#include <iostream>\n#include <vector>\n\n#include <cuda_runtime.h>\n\n#define CHECK_CUDA_ERROR(val) check((val), #val, __FILE__, __LINE__)\ntemplate <typename T>\nvoid check(T err, const char* const func, const char* const file,\n           const int line)\n{\n    if (err != cudaSuccess)\n    {\n        std::cerr << "CUDA Runtime Error at: " << file << ":" << line\n                  << std::endl;\n        std::cerr << cudaGetErrorString(err) << " " << func << std::endl;\n        std::exit(EXIT_FAILURE);\n    }\n}\n\n#define CHECK_LAST_CUDA_ERROR() checkLast(__FILE__, __LINE__)\nvoid checkLast(const char* const file, const int line)\n{\n    cudaError_t err{cudaGetLastError()};\n    if (err != cudaSuccess)\n    {\n        std::cerr << "CUDA Runtime Error at: " << file << ":" << line\n                  << std::endl;\n        std::cerr << cudaGetErrorString(err) << std::endl;\n        std::exit(EXIT_FAILURE);\n    }\n}\n\n__global__ void add_val_in_place(int32_t* data, int32_t val, uint32_t n)\n{\n    uint32_t const idx{blockDim.x * blockIdx.x + threadIdx.x};\n    uint32_t const stride{blockDim.x * gridDim.x};\n    for (uint32_t i{idx}; i < n; i += stride)\n    {\n        data[i] += val;\n    }\n}\n\nvoid launch_add_val_in_place(int32_t* data, int32_t val, uint32_t n,\n                             cudaStream_t stream)\n{\n    dim3 const threads_per_block{1024};\n    dim3 const blocks_per_grid{32};\n    add_val_in_place<<<blocks_per_grid, threads_per_block, 0, stream>>>(data,\n                                                                        val, n);\n    CHECK_LAST_CUDA_ERROR();\n}\n\nbool check_array_value(int32_t const* data, uint32_t n, int32_t val)\n{\n    for (uint32_t i{0}; i < n; ++i)\n    {\n        if (data[i] != val)\n        {\n            return false;\n        }\n    }\n    return true;\n}\n\nint main()\n{\n    constexpr uint32_t const n{1000000};\n    constexpr int32_t const val_1{1};\n    constexpr int32_t const val_2{2};\n    constexpr int32_t const val_3{3};\n    // Create an multi-stream application.\n    cudaStream_t stream_1{0};\n    cudaStream_t stream_2{0};\n    // stream_1 is a non-default non-blocking stream.\n    CHECK_CUDA_ERROR(cudaStreamCreateWithFlags(&stream_1, cudaStreamNonBlocking));\n\n    std::vector<int32_t> vec(n, 0);\n    int32_t* d_data{nullptr};\n    CHECK_CUDA_ERROR(cudaMalloc(&d_data, n * sizeof(int32_t)));\n    CHECK_CUDA_ERROR(cudaMemcpy(d_data, vec.data(), n * sizeof(int32_t),\n                                cudaMemcpyHostToDevice));\n    // Run a sequence of CUDA kernels in order on the same CUDA stream.\n    launch_add_val_in_place(d_data, val_1, n, stream_1);\n    // The second kernel launch is supposed to be run on stream_1.\n    // However, the implementation has a typo so that the kernel launch\n    // is run on the default stream_2.\n    launch_add_val_in_place(d_data, val_2, n, stream_2);\n    launch_add_val_in_place(d_data, val_3, n, stream_1);\n\n    CHECK_CUDA_ERROR(cudaStreamSynchronize(stream_1));\n    CHECK_CUDA_ERROR(cudaMemcpy(vec.data(), d_data, n * sizeof(int32_t),\n                                cudaMemcpyDeviceToHost));\n\n    // Check the correctness of the application.\n    // Yet the result will still be correct if the default stream_2\n    // is a legacy default stream.\n    assert(check_array_value(vec.data(), n, val_1 + val_2 + val_3));\n\n    CHECK_CUDA_ERROR(cudaFree(d_data));\n    CHECK_CUDA_ERROR(cudaStreamDestroy(stream_1));\n}\n')))}u.isMDXComponent=!0}}]);