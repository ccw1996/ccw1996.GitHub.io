"use strict";(self.webpackChunkblog_sample=self.webpackChunkblog_sample||[]).push([[3778],{3905:(e,t,n)=>{n.d(t,{Zo:()=>p,kt:()=>d});var r=n(7294);function o(e,t,n){return t in e?Object.defineProperty(e,t,{value:n,enumerable:!0,configurable:!0,writable:!0}):e[t]=n,e}function i(e,t){var n=Object.keys(e);if(Object.getOwnPropertySymbols){var r=Object.getOwnPropertySymbols(e);t&&(r=r.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),n.push.apply(n,r)}return n}function a(e){for(var t=1;t<arguments.length;t++){var n=null!=arguments[t]?arguments[t]:{};t%2?i(Object(n),!0).forEach((function(t){o(e,t,n[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(n)):i(Object(n)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(n,t))}))}return e}function c(e,t){if(null==e)return{};var n,r,o=function(e,t){if(null==e)return{};var n,r,o={},i=Object.keys(e);for(r=0;r<i.length;r++)n=i[r],t.indexOf(n)>=0||(o[n]=e[n]);return o}(e,t);if(Object.getOwnPropertySymbols){var i=Object.getOwnPropertySymbols(e);for(r=0;r<i.length;r++)n=i[r],t.indexOf(n)>=0||Object.prototype.propertyIsEnumerable.call(e,n)&&(o[n]=e[n])}return o}var l=r.createContext({}),s=function(e){var t=r.useContext(l),n=t;return e&&(n="function"==typeof e?e(t):a(a({},t),e)),n},p=function(e){var t=s(e.components);return r.createElement(l.Provider,{value:t},e.children)},u="mdxType",m={inlineCode:"code",wrapper:function(e){var t=e.children;return r.createElement(r.Fragment,{},t)}},f=r.forwardRef((function(e,t){var n=e.components,o=e.mdxType,i=e.originalType,l=e.parentName,p=c(e,["components","mdxType","originalType","parentName"]),u=s(n),f=o,d=u["".concat(l,".").concat(f)]||u[f]||m[f]||i;return n?r.createElement(d,a(a({ref:t},p),{},{components:n})):r.createElement(d,a({ref:t},p))}));function d(e,t){var n=arguments,o=t&&t.mdxType;if("string"==typeof e||o){var i=n.length,a=new Array(i);a[0]=f;var c={};for(var l in t)hasOwnProperty.call(t,l)&&(c[l]=t[l]);c.originalType=e,c[u]="string"==typeof e?e:o,a[1]=c;for(var s=2;s<i;s++)a[s]=n[s];return r.createElement.apply(null,a)}return r.createElement.apply(null,n)}f.displayName="MDXCreateElement"},2443:(e,t,n)=>{n.r(t),n.d(t,{assets:()=>l,contentTitle:()=>a,default:()=>m,frontMatter:()=>i,metadata:()=>c,toc:()=>s});var r=n(7462),o=(n(7294),n(3905));const i={title:"TORCH.JIT.OPTIMIZE_FOR_INFERENCE",tags:["work"]},a=void 0,c={unversionedId:"ml/torch-optimize",id:"ml/torch-optimize",title:"TORCH.JIT.OPTIMIZE_FOR_INFERENCE",description:"\u6267\u884c\u4e00\u7ec4\u4f18\u5316\u8fc7\u7a0b\uff0c\u4ee5\u4f18\u5316\u6a21\u578b \u63a8\u7406\u7684\u76ee\u7684\u3002\u5982\u679c\u6a21\u578b\u5c1a\u672a\u51bb\u7ed3\uff0coptimizeforinference \u5c06\u81ea\u52a8\u8c03\u7528 torch.jit.freeze\u3002",source:"@site/docs/ml/torch-optimize.md",sourceDirName:"ml",slug:"/ml/torch-optimize",permalink:"/docs/ml/torch-optimize",draft:!1,editUrl:"https://github.com/facebook/docusaurus/edit/main/website/docs/ml/torch-optimize.md",tags:[{label:"work",permalink:"/docs/tags/work"}],version:"current",lastUpdatedAt:1699283718,formattedLastUpdatedAt:"Nov 6, 2023",frontMatter:{title:"TORCH.JIT.OPTIMIZE_FOR_INFERENCE",tags:["work"]},sidebar:"tutorialSidebar",previous:{title:"\u5377\u79ef\u795e\u7ecf\u7f51\u7edc(CNN)\u5f20\u91cf(\u56fe\u50cf)\u7684\u5c3a\u5bf8\u548c\u53c2\u6570\u8ba1\u7b97",permalink:"/docs/ml/size"},next:{title:"training",permalink:"/docs/ml/training/"}},l={},s=[],p={toc:s},u="wrapper";function m(e){let{components:t,...n}=e;return(0,o.kt)(u,(0,r.Z)({},p,n,{components:t,mdxType:"MDXLayout"}),(0,o.kt)("p",null,"\u6267\u884c\u4e00\u7ec4\u4f18\u5316\u8fc7\u7a0b\uff0c\u4ee5\u4f18\u5316\u6a21\u578b \u63a8\u7406\u7684\u76ee\u7684\u3002\u5982\u679c\u6a21\u578b\u5c1a\u672a\u51bb\u7ed3\uff0coptimize_for_inference \u5c06\u81ea\u52a8\u8c03\u7528 torch.jit.freeze\u3002\n\u8fd9\u4ecd\u5904\u4e8e\u539f\u578b\u9636\u6bb5\uff0c\u53ef\u80fd\u4f1a\u51cf\u6162\u6a21\u578b\u901f\u5ea6\u3002 \u5230\u76ee\u524d\u4e3a\u6b62\uff0c\u4e3b\u8981\u9488\u5bf9\u7684\u7528\u4f8b\u662f cpu \u4e0a\u7684\u89c6\u89c9\u6a21\u578b \u548c GPU \u5728\u8f83\u5c0f\u7a0b\u5ea6\u4e0a\u3002"),(0,o.kt)("p",null,"In addition to generic optimizations that should speed up your model regardless of environment, prepare for inference will also bake in build specific settings such as the presence of CUDNN or MKLDNN, and may in the future make transformations which speed things up on one machine but slow things down on another. "),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},'import torch\nin_channels, out_channels = 3, 32\nconv = torch.nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=2, bias=True)\nbn = torch.nn.BatchNorm2d(out_channels, eps=.001)\nmod = torch.nn.Sequential(conv, bn)\nfrozen_mod = torch.jit.optimize_for_inference(torch.jit.script(mod.eval()))\nassert "batch_norm" not in str(frozen_mod.graph)\n# if built with MKLDNN, convolution will be run with MKLDNN weights\nassert "MKLDNN" in frozen_mod.graph\n')))}m.isMDXComponent=!0}}]);